Sender: LSF System <lsfadmin@eu-lo-s4-009>
Subject: Job 226770750: <python SCAL_transformer.py --cfg configs/default_gpu.yaml> in cluster <euler> Exited

Job <python SCAL_transformer.py --cfg configs/default_gpu.yaml> was submitted from host <eu-login-20> by user <mezhang> in cluster <euler> at Thu Jul 28 09:13:32 2022
Job was executed on host(s) <4*eu-lo-s4-009>, in queue <gpu.120h>, as user <mezhang> in cluster <euler> at Thu Jul 28 09:14:00 2022
</cluster/home/mezhang> was used as the home directory.
</cluster/home/mezhang/twitter-sent-analysis> was used as the working directory.
Started at Thu Jul 28 09:14:00 2022
Terminated at Thu Jul 28 09:17:45 2022
Results reported at Thu Jul 28 09:17:45 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python SCAL_transformer.py --cfg configs/default_gpu.yaml
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 143.

Resource usage summary:

    CPU time :                                   294.73 sec.
    Max Memory :                                 25311 MB
    Average Memory :                             9905.50 MB
    Total Requested Memory :                     32768.00 MB
    Delta Memory :                               7457.00 MB
    Max Swap :                                   -
    Max Processes :                              7
    Max Threads :                                39
    Run time :                                   225 sec.
    Turnaround time :                            253 sec.

The output (if any) follows:

wandb: Currently logged in as: mezhang. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in /cluster/home/mezhang/twitter-sent-analysis/wandb/run-20220728_091529-2ahc5m6m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run celestial-elevator-32
wandb: ‚≠êÔ∏è View project at https://wandb.ai/mezhang/twitter-sentiment-analysis-scal
wandb: üöÄ View run at https://wandb.ai/mezhang/twitter-sentiment-analysis-scal/runs/2ahc5m6m
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/transformers/convert_slow_tokenizer.py:435: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  "The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option"
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.classifier.weight']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).

Running on the cluster.
The project path is:  /cluster/home/mezhang/twitter-sent-analysis/
The experiment path is:  /cluster/scratch/mezhang/Experiments/
The model checkpoints will be saved at:  /cluster/scratch/mezhang/Experiments/experiment-Wed_Feb_25_23h33m05s/checkpoints/ 

We will use 2500000 tweets in total. 2375000 for training and 125000 for validation.
1250000 tweets will be used for each of the 2 subset iterations (i.e., in each subset that is split in training/validation).

Running on cuda:0  with  1.12.0+cu102 

Going to iterate over 2 subsets of 1250000 samples/tweets (separated for training/validation) to see 2500000 in total.
Going to read 1250000 lines (625000 in each of the pos and neg datasets), starting_line:0, end_line:625000
Loaded 1250000 tweets!
Number of indices for training:  1187500
Number of indices for validation:  62500
Traceback (most recent call last):
  File "SCAL_transformer.py", line 472, in <module>
    model = run_training(model)
  File "SCAL_transformer.py", line 340, in run_training
    trained_model = load_and_train(model, amount_per_it, iteration)
  File "SCAL_transformer.py", line 283, in load_and_train
    train_dataset, val_dataset = get_train_val_data(tweets, labels)
  File "SCAL_transformer.py", line 171, in get_train_val_data
    X_train = tokenizer(X_train_list, max_length=128, padding="max_length", truncation=True)
  File "/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/transformers/tokenization_utils_base.py", line 2512, in __call__
    **kwargs,
  File "/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/transformers/tokenization_utils_base.py", line 2703, in batch_encode_plus
    **kwargs,
  File "/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/transformers/tokenization_utils_fast.py", line 449, in _batch_encode_plus
    for encoding in encodings
  File "/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/transformers/tokenization_utils_fast.py", line 449, in <listcomp>
    for encoding in encodings
  File "/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/transformers/tokenization_utils_fast.py", line 219, in _convert_encoding
    encoding_dict["input_ids"].append(e.ids)
KeyboardInterrupt
wandb: Waiting for W&B process to finish... (failed 255). Press Control-C to abort syncing.
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb:                                                                                
/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 6 leaked semaphores to clean up at shutdown
  len(cache))
Terminated
