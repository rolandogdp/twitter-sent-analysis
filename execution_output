Sender: LSF System <lsfadmin@eu-lo-s4-029>
Subject: Job 226349568: <python SCAL_transformer.py --cfg configs/default_gpu.yaml> in cluster <euler> Exited

Job <python SCAL_transformer.py --cfg configs/default_gpu.yaml> was submitted from host <eu-login-46> by user <mezhang> in cluster <euler> at Sat Jul 23 20:20:26 2022
Job was executed on host(s) <4*eu-lo-s4-029>, in queue <gpu.24h>, as user <mezhang> in cluster <euler> at Sat Jul 23 20:20:56 2022
</cluster/home/mezhang> was used as the home directory.
</cluster/home/mezhang/twitter-sent-analysis> was used as the working directory.
Started at Sat Jul 23 20:20:56 2022
Terminated at Sat Jul 23 20:22:51 2022
Results reported at Sat Jul 23 20:22:51 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python SCAL_transformer.py --cfg configs/default_gpu.yaml
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   107.55 sec.
    Max Memory :                                 6470 MB
    Average Memory :                             3382.33 MB
    Total Requested Memory :                     32768.00 MB
    Delta Memory :                               26298.00 MB
    Max Swap :                                   -
    Max Processes :                              7
    Max Threads :                                50
    Run time :                                   127 sec.
    Turnaround time :                            145 sec.

The output (if any) follows:

wandb: Currently logged in as: mezhang. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in /cluster/home/mezhang/twitter-sent-analysis/wandb/run-20220723_202139-6t2n1mz0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pretty-elevator-9
wandb: ‚≠êÔ∏è View project at https://wandb.ai/mezhang/twitter-sentiment-analysis-scal
wandb: üöÄ View run at https://wandb.ai/mezhang/twitter-sentiment-analysis-scal/runs/6t2n1mz0
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/transformers/convert_slow_tokenizer.py:435: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  "The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option"
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
***** Running training *****
  Num examples = 237500
  Num Epochs = 2
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 4
  Total optimization steps = 7422
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"

Running on the cluster.
The project path is:  /cluster/home/mezhang/twitter-sent-analysis/
The experiment path is:  /cluster/scratch/mezhang/Experiments/
The model checkpoints will be saved at:  /cluster/scratch/mezhang/Experiments/experiment-Sun_Jan_18_09h13m51s/checkpoints/ 

We will use 250000 tweets in total. 237500 for training and 12500 for validation.
250000 tweets will be used for each of the 1 subset iterations (i.e., in each subset that is split in training/validation).

Running on cuda:0  with  1.12.0+cu102 

Going to iterate over 1 subsets of 250000 samples/tweets (separated for training/validation) to see 250000 in total.
Going to read 250000 lines (125000 in each of the pos and neg datasets), starting_line:0, end_line:125000
Loaded 250000 tweets!
Number of indices for training:  237500
Number of indices for validation:  12500
  0% 0/7422 [00:00<?, ?it/s]/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/torch/nn/modules/module.py:1894: UserWarning: Calling .zero_grad() from a module created with nn.DataParallel() has no effect. The parameters are copied (in a differentiable manner) from the original module. This means they are not leaf nodes in autograd and so don't accumulate gradients. If you need gradients in your forward method, consider using autograd.grad instead.
  "Calling .zero_grad() from a module created with nn.DataParallel() has no effect. "
/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0% 1/7422 [00:11<22:45:26, 11.04s/it]/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/torch/autograd/__init__.py:175: UserWarning: Error detected in BroadcastBackward. Traceback of forward call that caused the error:
  File "SCAL_transformer.py", line 472, in <module>
    model = run_training(model)
  File "SCAL_transformer.py", line 340, in run_training
    trained_model = load_and_train(model, amount_per_it, iteration)
  File "SCAL_transformer.py", line 291, in load_and_train
    train(model, train_dataset, val_dataset)
  File "SCAL_transformer.py", line 237, in train
    trainer.train()
  File "/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/transformers/trainer.py", line 1413, in train
    ignore_keys_for_eval=ignore_keys_for_eval,
  File "/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/transformers/trainer.py", line 1651, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/transformers/trainer.py", line 2345, in training_step
    loss = self.compute_loss(model, inputs)
  File "/cluster/home/mezhang/twitter-sent-analysis/trainers/myScalTrainer.py", line 12, in compute_loss
    outputs = model(**inputs, return_dict = True)
  File "/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 167, in forward
    replicas = self.replicate(self.module, self.device_ids[:len(inputs)])
  File "/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 172, in replicate
    return replicate(module, device_ids, not torch.is_grad_enabled())
  File "/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/torch/nn/parallel/replicate.py", line 91, in replicate
    param_copies = _broadcast_coalesced_reshape(params, devices, detach)
  File "/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/torch/nn/parallel/replicate.py", line 71, in _broadcast_coalesced_reshape
    tensor_copies = Broadcast.apply(devices, *tensors)
 (Triggered internally at  ../torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
GOT ERROR: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py", line 61, in _worker
    output = module(*input, **kwargs)
  File "/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/cluster/home/mezhang/twitter-sent-analysis/models/myScalModel.py", line 161, in forward
    loss.backward(retain_graph = True);
  File "/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/torch/autograd/__init__.py", line 175, in backward
    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
  File "/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/torch/autograd/function.py", line 253, in apply
    return user_fn(self, *args)
  File "/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/torch/nn/parallel/_functions.py", line 34, in backward
    return (None,) + ReduceAddCoalesced.apply(ctx.input_device, ctx.num_inputs, *grad_outputs)
  File "/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/torch/nn/parallel/_functions.py", line 45, in forward
    return comm.reduce_add_coalesced(grads_, destination)
  File "/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/torch/nn/parallel/comm.py", line 143, in reduce_add_coalesced
    flat_result = reduce_add(flat_tensors, destination)
  File "/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/torch/nn/parallel/comm.py", line 95, in reduce_add
    result = torch.empty_like(inputs[root_index])
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 7.93 GiB total capacity; 7.08 GiB already allocated; 8.31 MiB free; 7.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

Traceback (most recent call last):
  File "SCAL_transformer.py", line 472, in <module>
    model = run_training(model)
  File "SCAL_transformer.py", line 348, in run_training
    raise(e)
  File "SCAL_transformer.py", line 340, in run_training
    trained_model = load_and_train(model, amount_per_it, iteration)
  File "SCAL_transformer.py", line 291, in load_and_train
    train(model, train_dataset, val_dataset)
  File "SCAL_transformer.py", line 237, in train
    trainer.train()
  File "/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/transformers/trainer.py", line 1413, in train
    ignore_keys_for_eval=ignore_keys_for_eval,
  File "/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/transformers/trainer.py", line 1651, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/transformers/trainer.py", line 2345, in training_step
    loss = self.compute_loss(model, inputs)
  File "/cluster/home/mezhang/twitter-sent-analysis/trainers/myScalTrainer.py", line 12, in compute_loss
    outputs = model(**inputs, return_dict = True)
  File "/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 168, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 178, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py", line 86, in parallel_apply
    output.reraise()
  File "/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/torch/_utils.py", line 461, in reraise
    raise exception
RuntimeError: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py", line 61, in _worker
    output = module(*input, **kwargs)
  File "/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/cluster/home/mezhang/twitter-sent-analysis/models/myScalModel.py", line 161, in forward
    loss.backward(retain_graph = True);
  File "/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/torch/autograd/__init__.py", line 175, in backward
    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
  File "/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/torch/autograd/function.py", line 253, in apply
    return user_fn(self, *args)
  File "/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/torch/nn/parallel/_functions.py", line 34, in backward
    return (None,) + ReduceAddCoalesced.apply(ctx.input_device, ctx.num_inputs, *grad_outputs)
  File "/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/torch/nn/parallel/_functions.py", line 45, in forward
    return comm.reduce_add_coalesced(grads_, destination)
  File "/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/torch/nn/parallel/comm.py", line 143, in reduce_add_coalesced
    flat_result = reduce_add(flat_tensors, destination)
  File "/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/torch/nn/parallel/comm.py", line 95, in reduce_add
    result = torch.empty_like(inputs[root_index])
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 7.93 GiB total capacity; 7.08 GiB already allocated; 8.31 MiB free; 7.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.015 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: | 0.013 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: / 0.015 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: Synced pretty-elevator-9: https://wandb.ai/mezhang/twitter-sentiment-analysis-scal/runs/6t2n1mz0
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220723_202139-6t2n1mz0/logs
Sender: LSF System <lsfadmin@eu-g3-045>
Subject: Job 226349668: <python SCAL_transformer.py --cfg configs/default_gpu.yaml> in cluster <euler> Exited

Job <python SCAL_transformer.py --cfg configs/default_gpu.yaml> was submitted from host <eu-login-46> by user <mezhang> in cluster <euler> at Sat Jul 23 20:24:57 2022
Job was executed on host(s) <4*eu-g3-045>, in queue <gpu.24h>, as user <mezhang> in cluster <euler> at Sat Jul 23 20:25:25 2022
</cluster/home/mezhang> was used as the home directory.
</cluster/home/mezhang/twitter-sent-analysis> was used as the working directory.
Started at Sat Jul 23 20:25:25 2022
Terminated at Sat Jul 23 20:40:46 2022
Results reported at Sat Jul 23 20:40:46 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python SCAL_transformer.py --cfg configs/default_gpu.yaml
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 143.

Resource usage summary:

    CPU time :                                   920.39 sec.
    Max Memory :                                 8143 MB
    Average Memory :                             7706.79 MB
    Total Requested Memory :                     32768.00 MB
    Delta Memory :                               24625.00 MB
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                37
    Run time :                                   921 sec.
    Turnaround time :                            949 sec.

The output (if any) follows:

wandb: Currently logged in as: mezhang. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in /cluster/home/mezhang/twitter-sent-analysis/wandb/run-20220723_202537-1x8y0jxo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run devoted-music-10
wandb: ‚≠êÔ∏è View project at https://wandb.ai/mezhang/twitter-sentiment-analysis-scal
wandb: üöÄ View run at https://wandb.ai/mezhang/twitter-sentiment-analysis-scal/runs/1x8y0jxo
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/transformers/convert_slow_tokenizer.py:435: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  "The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option"
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
***** Running training *****
  Num examples = 237500
  Num Epochs = 2
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 1
  Total optimization steps = 29688
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"

Running on the cluster.
The project path is:  /cluster/home/mezhang/twitter-sent-analysis/
The experiment path is:  /cluster/scratch/mezhang/Experiments/
The model checkpoints will be saved at:  /cluster/scratch/mezhang/Experiments/experiment-Sun_Feb__8_06h39m53s/checkpoints/ 

We will use 250000 tweets in total. 237500 for training and 12500 for validation.
250000 tweets will be used for each of the 1 subset iterations (i.e., in each subset that is split in training/validation).

Running on cuda:0  with  1.12.0+cu102 

Going to iterate over 1 subsets of 250000 samples/tweets (separated for training/validation) to see 250000 in total.
Going to read 250000 lines (125000 in each of the pos and neg datasets), starting_line:0, end_line:125000
Loaded 250000 tweets!
Number of indices for training:  237500
Number of indices for validation:  12500
  0% 0/29688 [00:00<?, ?it/s]  0% 1/29688 [00:02<17:10:17,  2.08s/it]  0% 2/29688 [00:02<11:30:13,  1.40s/it]  0% 3/29688 [00:03<9:42:10,  1.18s/it]   0% 4/29688 [00:04<8:50:48,  1.07s/it]  0% 5/29688 [00:05<8:23:00,  1.02s/it]  0% 6/29688 [00:06<8:06:49,  1.02it/s]  0% 7/29688 [00:07<7:56:03,  1.04it/s]  0% 8/29688 [00:08<7:47:52,  1.06it/s]  0% 9/29688 [00:09<7:42:39,  1.07it/s]  0% 10/29688 [00:10<7:38:57,  1.08it/s]  0% 11/29688 [00:11<7:35:33,  1.09it/s]  0% 12/29688 [00:12<7:33:38,  1.09it/s]  0% 13/29688 [00:13<7:34:13,  1.09it/s]  0% 14/29688 [00:13<7:32:47,  1.09it/s]  0% 15/29688 [00:14<7:32:27,  1.09it/s]  0% 16/29688 [00:15<7:31:54,  1.09it/s]  0% 17/29688 [00:16<7:30:46,  1.10it/s]  0% 18/29688 [00:17<7:29:41,  1.10it/s]  0% 19/29688 [00:18<7:29:41,  1.10it/s]  0% 20/29688 [00:19<7:29:48,  1.10it/s]  0% 21/29688 [00:20<7:31:12,  1.10it/s]  0% 22/29688 [00:21<7:32:21,  1.09it/s]  0% 23/29688 [00:22<7:31:18,  1.10it/s]  0% 24/29688 [00:23<7:32:14,  1.09it/s]  0% 25/29688 [00:23<7:30:26,  1.10it/s]  0% 26/29688 [00:24<7:30:12,  1.10it/s]  0% 27/29688 [00:25<7:29:21,  1.10it/s]  0% 28/29688 [00:26<7:29:49,  1.10it/s]  0% 29/29688 [00:27<7:29:28,  1.10it/s]  0% 30/29688 [00:28<7:28:36,  1.10it/s]  0% 31/29688 [00:29<7:29:11,  1.10it/s]  0% 32/29688 [00:30<7:28:03,  1.10it/s]  0% 33/29688 [00:31<7:28:04,  1.10it/s]  0% 34/29688 [00:32<7:31:28,  1.09it/s]  0% 35/29688 [00:33<7:31:59,  1.09it/s]  0% 36/29688 [00:34<7:34:06,  1.09it/s]  0% 37/29688 [00:34<7:35:01,  1.09it/s]  0% 38/29688 [00:35<7:33:28,  1.09it/s]  0% 39/29688 [00:36<7:33:54,  1.09it/s]  0% 40/29688 [00:37<7:33:09,  1.09it/s]  0% 41/29688 [00:38<7:33:08,  1.09it/s]  0% 42/29688 [00:39<7:31:46,  1.09it/s]  0% 43/29688 [00:40<7:32:29,  1.09it/s]  0% 44/29688 [00:41<7:32:10,  1.09it/s]  0% 45/29688 [00:42<7:31:18,  1.09it/s]  0% 46/29688 [00:43<7:31:57,  1.09it/s]  0% 47/29688 [00:44<7:32:39,  1.09it/s]  0% 48/29688 [00:45<7:31:43,  1.09it/s]  0% 49/29688 [00:45<7:31:32,  1.09it/s]  0% 50/29688 [00:46<7:32:09,  1.09it/s]  0% 51/29688 [00:47<7:31:08,  1.09it/s]  0% 52/29688 [00:48<7:30:43,  1.10it/s]  0% 53/29688 [00:49<7:30:02,  1.10it/s]  0% 54/29688 [00:50<7:31:07,  1.09it/s]  0% 55/29688 [00:51<7:34:08,  1.09it/s]  0% 56/29688 [00:52<7:34:34,  1.09it/s]  0% 57/29688 [00:53<7:34:04,  1.09it/s]  0% 58/29688 [00:54<7:33:21,  1.09it/s]  0% 59/29688 [00:55<7:32:28,  1.09it/s]  0% 60/29688 [00:56<7:32:47,  1.09it/s]  0% 61/29688 [00:56<7:31:59,  1.09it/s]  0% 62/29688 [00:57<7:31:34,  1.09it/s]  0% 63/29688 [00:58<7:30:36,  1.10it/s]  0% 64/29688 [00:59<7:29:48,  1.10it/s]  0% 65/29688 [01:00<7:30:42,  1.10it/s]  0% 66/29688 [01:01<7:30:45,  1.10it/s]  0% 67/29688 [01:02<7:30:18,  1.10it/s]  0% 68/29688 [01:03<7:30:03,  1.10it/s]  0% 69/29688 [01:04<7:29:57,  1.10it/s]  0% 70/29688 [01:05<7:29:32,  1.10it/s]  0% 71/29688 [01:06<7:29:30,  1.10it/s]  0% 72/29688 [01:06<7:28:22,  1.10it/s]  0% 73/29688 [01:07<7:28:52,  1.10it/s]  0% 74/29688 [01:08<7:28:52,  1.10it/s]  0% 75/29688 [01:09<7:29:46,  1.10it/s]  0% 76/29688 [01:10<7:30:00,  1.10it/s]  0% 77/29688 [01:11<7:31:13,  1.09it/s]  0% 78/29688 [01:12<7:32:22,  1.09it/s]  0% 79/29688 [01:13<7:31:54,  1.09it/s]  0% 80/29688 [01:14<7:31:55,  1.09it/s]  0% 81/29688 [01:15<7:31:31,  1.09it/s]  0% 82/29688 [01:16<7:31:06,  1.09it/s]  0% 83/29688 [01:16<7:30:35,  1.10it/s]  0% 84/29688 [01:17<7:31:57,  1.09it/s]  0% 85/29688 [01:18<7:31:42,  1.09it/s]  0% 86/29688 [01:19<7:33:17,  1.09it/s]  0% 87/29688 [01:20<7:31:54,  1.09it/s]  0% 88/29688 [01:21<7:32:18,  1.09it/s]  0% 89/29688 [01:22<7:32:42,  1.09it/s]  0% 90/29688 [01:23<7:32:57,  1.09it/s]  0% 91/29688 [01:24<7:31:55,  1.09it/s]  0% 92/29688 [01:25<7:33:01,  1.09it/s]  0% 93/29688 [01:26<7:33:30,  1.09it/s]  0% 94/29688 [01:27<7:32:38,  1.09it/s]  0% 95/29688 [01:27<7:31:48,  1.09it/s]  0% 96/29688 [01:28<7:32:02,  1.09it/s]  0% 97/29688 [01:29<7:32:46,  1.09it/s]  0% 98/29688 [01:30<7:32:55,  1.09it/s]  0% 99/29688 [01:31<7:32:43,  1.09it/s]  0% 100/29688 [01:32<7:32:42,  1.09it/s]  0% 101/29688 [01:33<7:33:28,  1.09it/s]  0% 102/29688 [01:34<7:32:23,  1.09it/s]  0% 103/29688 [01:35<7:31:26,  1.09it/s]  0% 104/29688 [01:36<7:31:32,  1.09it/s]  0% 105/29688 [01:37<7:31:16,  1.09it/s]  0% 106/29688 [01:38<7:30:21,  1.09it/s]  0% 107/29688 [01:38<7:29:59,  1.10it/s]  0% 108/29688 [01:39<7:29:51,  1.10it/s]  0% 109/29688 [01:40<7:29:56,  1.10it/s]  0% 110/29688 [01:41<7:29:38,  1.10it/s]  0% 111/29688 [01:42<7:29:34,  1.10it/s]  0% 112/29688 [01:43<7:30:04,  1.10it/s]  0% 113/29688 [01:44<7:29:36,  1.10it/s]  0% 114/29688 [01:45<7:29:34,  1.10it/s]  0% 115/29688 [01:47<10:19:37,  1.26s/it]  0% 116/29688 [01:48<9:29:06,  1.15s/it]   0% 117/29688 [01:49<8:53:04,  1.08s/it]  0% 118/29688 [01:50<8:27:26,  1.03s/it]  0% 119/29688 [01:51<8:10:10,  1.01it/s]  0% 120/29688 [01:51<7:58:06,  1.03it/s]  0% 121/29688 [01:52<7:49:53,  1.05it/s]  0% 122/29688 [01:53<7:45:28,  1.06it/s]  0% 123/29688 [01:54<7:40:24,  1.07it/s]  0% 124/29688 [01:55<7:36:57,  1.08it/s]  0% 125/29688 [01:56<7:35:51,  1.08it/s]  0% 126/29688 [01:57<7:34:42,  1.08it/s]  0% 127/29688 [01:58<7:32:56,  1.09it/s]  0% 128/29688 [01:59<7:31:24,  1.09it/s]  0% 129/29688 [02:00<7:32:07,  1.09it/s]  0% 130/29688 [02:01<7:31:37,  1.09it/s]  0% 131/29688 [02:02<7:30:56,  1.09it/s]  0% 132/29688 [02:02<7:31:42,  1.09it/s]  0% 133/29688 [02:03<7:31:15,  1.09it/s]  0% 134/29688 [02:04<7:31:01,  1.09it/s]  0% 135/29688 [02:05<7:30:21,  1.09it/s]  0% 136/29688 [02:06<7:29:52,  1.09it/s]  0% 137/29688 [02:07<7:30:51,  1.09it/s]  0% 138/29688 [02:08<7:30:20,  1.09it/s]  0% 139/29688 [02:09<7:29:35,  1.10it/s]  0% 140/29688 [02:10<7:29:04,  1.10it/s]  0% 141/29688 [02:11<7:29:55,  1.09it/s]  0% 142/29688 [02:12<7:29:04,  1.10it/s]  0% 143/29688 [02:13<7:30:34,  1.09it/s]  0% 144/29688 [02:13<7:29:41,  1.09it/s]  0% 145/29688 [02:14<7:29:16,  1.10it/s]  0% 146/29688 [02:15<7:28:40,  1.10it/s]  0% 147/29688 [02:16<7:28:24,  1.10it/s]  0% 148/29688 [02:17<7:28:50,  1.10it/s]  1% 149/29688 [02:18<7:29:42,  1.09it/s]  1% 150/29688 [02:19<7:29:38,  1.09it/s]  1% 151/29688 [02:20<7:29:01,  1.10it/s]  1% 152/29688 [02:21<7:28:30,  1.10it/s]  1% 153/29688 [02:22<7:28:11,  1.10it/s]  1% 154/29688 [02:23<7:28:05,  1.10it/s]  1% 155/29688 [02:23<7:28:18,  1.10it/s]  1% 156/29688 [02:24<7:27:38,  1.10it/s]  1% 157/29688 [02:25<7:28:13,  1.10it/s]  1% 158/29688 [02:26<7:28:52,  1.10it/s]  1% 159/29688 [02:27<7:28:27,  1.10it/s]  1% 160/29688 [02:28<7:28:54,  1.10it/s]  1% 161/29688 [02:29<7:29:06,  1.10it/s]  1% 162/29688 [02:30<7:28:08,  1.10it/s]  1% 163/29688 [02:31<7:29:38,  1.09it/s]  1% 164/29688 [02:32<7:28:58,  1.10it/s]  1% 165/29688 [02:33<7:30:25,  1.09it/s]  1% 166/29688 [02:34<7:30:44,  1.09it/s]  1% 167/29688 [02:34<7:30:03,  1.09it/s]  1% 168/29688 [02:35<7:30:31,  1.09it/s]  1% 169/29688 [02:36<7:30:48,  1.09it/s]  1% 170/29688 [02:37<7:30:13,  1.09it/s]  1% 171/29688 [02:38<7:29:54,  1.09it/s]  1% 172/29688 [02:39<7:29:52,  1.09it/s]  1% 173/29688 [02:40<7:28:51,  1.10it/s]  1% 174/29688 [02:41<7:27:41,  1.10it/s]  1% 175/29688 [02:42<7:27:37,  1.10it/s]  1% 176/29688 [02:43<7:27:23,  1.10it/s]  1% 177/29688 [02:44<7:29:27,  1.09it/s]  1% 178/29688 [02:44<7:28:37,  1.10it/s]  1% 179/29688 [02:45<7:28:24,  1.10it/s]  1% 180/29688 [02:46<7:28:10,  1.10it/s]  1% 181/29688 [02:47<7:28:18,  1.10it/s]  1% 182/29688 [02:48<7:29:18,  1.09it/s]  1% 183/29688 [02:49<7:28:36,  1.10it/s]  1% 184/29688 [02:50<7:29:17,  1.09it/s]  1% 185/29688 [02:51<7:29:00,  1.10it/s]  1% 186/29688 [02:52<7:28:26,  1.10it/s]  1% 187/29688 [02:53<7:28:22,  1.10it/s]  1% 188/29688 [02:54<7:27:54,  1.10it/s]  1% 189/29688 [02:54<7:27:56,  1.10it/s]  1% 190/29688 [02:55<7:27:45,  1.10it/s]  1% 191/29688 [02:56<7:27:35,  1.10it/s]  1% 192/29688 [02:57<7:27:25,  1.10it/s]  1% 193/29688 [02:58<7:27:19,  1.10it/s]  1% 194/29688 [02:59<7:27:46,  1.10it/s]  1% 195/29688 [03:00<7:27:32,  1.10it/s]  1% 196/29688 [03:01<7:28:37,  1.10it/s]  1% 197/29688 [03:02<7:28:23,  1.10it/s]  1% 198/29688 [03:03<7:27:29,  1.10it/s]  1% 199/29688 [03:04<7:27:58,  1.10it/s]  1% 200/29688 [03:05<7:27:33,  1.10it/s]  1% 201/29688 [03:05<7:27:56,  1.10it/s]  1% 202/29688 [03:06<7:27:19,  1.10it/s]  1% 203/29688 [03:07<7:27:13,  1.10it/s]  1% 204/29688 [03:08<7:27:58,  1.10it/s]  1% 205/29688 [03:09<7:27:21,  1.10it/s]  1% 206/29688 [03:10<7:26:51,  1.10it/s]  1% 207/29688 [03:11<7:26:10,  1.10it/s]  1% 208/29688 [03:12<7:27:11,  1.10it/s]  1% 209/29688 [03:13<7:27:15,  1.10it/s]  1% 210/29688 [03:14<7:27:12,  1.10it/s]  1% 211/29688 [03:15<7:28:02,  1.10it/s]  1% 212/29688 [03:15<7:27:44,  1.10it/s]  1% 213/29688 [03:16<7:27:15,  1.10it/s]  1% 214/29688 [03:17<7:25:45,  1.10it/s]  1% 215/29688 [03:18<7:25:10,  1.10it/s]  1% 216/29688 [03:19<7:24:41,  1.10it/s]  1% 217/29688 [03:20<7:25:11,  1.10it/s]  1% 218/29688 [03:21<7:24:19,  1.11it/s]  1% 219/29688 [03:22<7:23:37,  1.11it/s]  1% 220/29688 [03:23<7:24:00,  1.11it/s]  1% 221/29688 [03:24<7:24:01,  1.11it/s]  1% 222/29688 [03:24<7:24:29,  1.10it/s]  1% 223/29688 [03:25<7:25:06,  1.10it/s]  1% 224/29688 [03:26<7:25:46,  1.10it/s]  1% 225/29688 [03:27<7:26:01,  1.10it/s]  1% 226/29688 [03:28<7:25:11,  1.10it/s]  1% 227/29688 [03:29<7:25:25,  1.10it/s]  1% 228/29688 [03:30<7:25:25,  1.10it/s]  1% 229/29688 [03:32<10:06:50,  1.24s/it]  1% 230/29688 [03:33<9:18:32,  1.14s/it]   1% 231/29688 [03:34<8:45:09,  1.07s/it]  1% 232/29688 [03:35<8:21:29,  1.02s/it]  1% 233/29688 [03:36<8:03:58,  1.01it/s]  1% 234/29688 [03:36<7:52:30,  1.04it/s]  1% 235/29688 [03:37<7:44:38,  1.06it/s]  1% 236/29688 [03:38<7:38:59,  1.07it/s]  1% 237/29688 [03:39<7:35:43,  1.08it/s]  1% 238/29688 [03:40<7:33:05,  1.08it/s]  1% 239/29688 [03:41<7:30:11,  1.09it/s]  1% 240/29688 [03:42<7:29:59,  1.09it/s]  1% 241/29688 [03:43<7:28:53,  1.09it/s]  1% 242/29688 [03:44<7:28:03,  1.10it/s]  1% 243/29688 [03:45<7:28:50,  1.09it/s]  1% 244/29688 [03:46<7:28:06,  1.10it/s]  1% 245/29688 [03:46<7:27:58,  1.10it/s]  1% 246/29688 [03:47<7:26:58,  1.10it/s]  1% 247/29688 [03:48<7:27:06,  1.10it/s]  1% 248/29688 [03:49<7:29:35,  1.09it/s]  1% 249/29688 [03:50<7:29:21,  1.09it/s]  1% 250/29688 [03:51<7:28:20,  1.09it/s]  1% 251/29688 [03:52<7:28:08,  1.09it/s]  1% 252/29688 [03:53<7:27:30,  1.10it/s]  1% 253/29688 [03:54<7:27:05,  1.10it/s]  1% 254/29688 [03:55<7:26:41,  1.10it/s]  1% 255/29688 [03:56<7:26:04,  1.10it/s]  1% 256/29688 [03:57<7:25:13,  1.10it/s]  1% 257/29688 [03:57<7:25:51,  1.10it/s]  1% 258/29688 [03:58<7:26:42,  1.10it/s]  1% 259/29688 [03:59<7:26:56,  1.10it/s]  1% 260/29688 [04:00<7:26:59,  1.10it/s]  1% 261/29688 [04:01<7:28:32,  1.09it/s]  1% 262/29688 [04:02<7:28:26,  1.09it/s]  1% 263/29688 [04:03<7:27:56,  1.09it/s]  1% 264/29688 [04:04<7:27:31,  1.10it/s]  1% 265/29688 [04:05<7:27:14,  1.10it/s]  1% 266/29688 [04:06<7:26:52,  1.10it/s]  1% 267/29688 [04:07<7:26:53,  1.10it/s]  1% 268/29688 [04:07<7:26:07,  1.10it/s]  1% 269/29688 [04:08<7:26:27,  1.10it/s]  1% 270/29688 [04:09<7:25:12,  1.10it/s]  1% 271/29688 [04:10<7:25:57,  1.10it/s]  1% 272/29688 [04:11<7:26:43,  1.10it/s]  1% 273/29688 [04:12<7:26:44,  1.10it/s]  1% 274/29688 [04:13<7:26:07,  1.10it/s]  1% 275/29688 [04:14<7:25:10,  1.10it/s]  1% 276/29688 [04:15<7:26:30,  1.10it/s]  1% 277/29688 [04:16<7:26:40,  1.10it/s]  1% 278/29688 [04:17<7:26:34,  1.10it/s]  1% 279/29688 [04:17<7:26:12,  1.10it/s]  1% 280/29688 [04:18<7:26:02,  1.10it/s]  1% 281/29688 [04:19<7:27:25,  1.10it/s]  1% 282/29688 [04:20<7:28:06,  1.09it/s]  1% 283/29688 [04:21<7:26:47,  1.10it/s]  1% 284/29688 [04:22<7:26:07,  1.10it/s]  1% 285/29688 [04:23<7:25:59,  1.10it/s]  1% 286/29688 [04:24<7:25:43,  1.10it/s]  1% 287/29688 [04:25<7:25:02,  1.10it/s]  1% 288/29688 [04:26<7:24:42,  1.10it/s]  1% 289/29688 [04:27<7:25:27,  1.10it/s]  1% 290/29688 [04:27<7:24:12,  1.10it/s]  1% 291/29688 [04:28<7:24:22,  1.10it/s]  1% 292/29688 [04:29<7:24:49,  1.10it/s]  1% 293/29688 [04:30<7:24:17,  1.10it/s]  1% 294/29688 [04:31<7:24:18,  1.10it/s]  1% 295/29688 [04:32<7:24:45,  1.10it/s]  1% 296/29688 [04:33<7:24:51,  1.10it/s]  1% 297/29688 [04:34<7:25:02,  1.10it/s]  1% 298/29688 [04:35<7:25:15,  1.10it/s]  1% 299/29688 [04:36<7:24:16,  1.10it/s]  1% 300/29688 [04:37<7:24:19,  1.10it/s]  1% 301/29688 [04:37<7:23:38,  1.10it/s]  1% 302/29688 [04:38<7:23:45,  1.10it/s]  1% 303/29688 [04:39<7:23:34,  1.10it/s]  1% 304/29688 [04:40<7:26:40,  1.10it/s]  1% 305/29688 [04:41<7:26:18,  1.10it/s]  1% 306/29688 [04:42<7:26:10,  1.10it/s]  1% 307/29688 [04:43<7:25:26,  1.10it/s]  1% 308/29688 [04:44<7:27:06,  1.10it/s]  1% 309/29688 [04:45<7:26:38,  1.10it/s]  1% 310/29688 [04:46<7:26:36,  1.10it/s]  1% 311/29688 [04:47<7:27:45,  1.09it/s]  1% 312/29688 [04:47<7:26:02,  1.10it/s]  1% 313/29688 [04:48<7:26:54,  1.10it/s]  1% 314/29688 [04:49<7:26:34,  1.10it/s]  1% 315/29688 [04:50<7:26:03,  1.10it/s]  1% 316/29688 [04:51<7:25:56,  1.10it/s]  1% 317/29688 [04:52<7:25:53,  1.10it/s]  1% 318/29688 [04:53<7:25:21,  1.10it/s]  1% 319/29688 [04:54<7:25:33,  1.10it/s]  1% 320/29688 [04:55<7:25:27,  1.10it/s]  1% 321/29688 [04:56<7:25:49,  1.10it/s]  1% 322/29688 [04:57<7:25:33,  1.10it/s]  1% 323/29688 [04:58<7:25:25,  1.10it/s]  1% 324/29688 [05:00<10:07:01,  1.24s/it]  1% 325/29688 [05:00<9:18:53,  1.14s/it]   1% 326/29688 [05:01<8:45:56,  1.07s/it]  1% 327/29688 [05:02<8:21:13,  1.02s/it]  1% 328/29688 [05:03<8:04:17,  1.01it/s]  1% 329/29688 [05:04<7:53:14,  1.03it/s]  1% 330/29688 [05:05<7:46:03,  1.05it/s]  1% 331/29688 [05:06<7:39:42,  1.06it/s]  1% 332/29688 [05:07<7:34:31,  1.08it/s]  1% 333/29688 [05:08<7:31:46,  1.08it/s]  1% 334/29688 [05:09<7:29:18,  1.09it/s]  1% 335/29688 [05:10<7:28:04,  1.09it/s]  1% 336/29688 [05:10<7:28:39,  1.09it/s]  1% 337/29688 [05:11<7:27:16,  1.09it/s]  1% 338/29688 [05:12<7:26:30,  1.10it/s]  1% 339/29688 [05:13<7:26:08,  1.10it/s]  1% 340/29688 [05:14<7:25:51,  1.10it/s]  1% 341/29688 [05:15<7:27:14,  1.09it/s]  1% 342/29688 [05:16<7:26:11,  1.10it/s]  1% 343/29688 [05:17<7:25:45,  1.10it/s]  1% 344/29688 [05:18<7:25:04,  1.10it/s]  1% 345/29688 [05:19<7:24:55,  1.10it/s]  1% 346/29688 [05:20<7:24:25,  1.10it/s]  1% 347/29688 [05:20<7:25:05,  1.10it/s]  1% 348/29688 [05:21<7:24:52,  1.10it/s]  1% 349/29688 [05:22<7:27:52,  1.09it/s]  1% 350/29688 [05:23<7:28:19,  1.09it/s]  1% 351/29688 [05:24<7:30:06,  1.09it/s]  1% 352/29688 [05:25<7:28:36,  1.09it/s]  1% 353/29688 [05:26<7:29:30,  1.09it/s]  1% 354/29688 [05:27<7:28:23,  1.09it/s]  1% 355/29688 [05:28<7:28:02,  1.09it/s]  1% 356/29688 [05:29<7:27:16,  1.09it/s]  1% 357/29688 [05:30<7:26:23,  1.10it/s]  1% 358/29688 [05:31<7:27:05,  1.09it/s]  1% 359/29688 [05:31<7:27:51,  1.09it/s]  1% 360/29688 [05:32<7:29:00,  1.09it/s]  1% 361/29688 [05:33<7:28:27,  1.09it/s]  1% 362/29688 [05:34<7:28:48,  1.09it/s]  1% 363/29688 [05:35<7:28:10,  1.09it/s]  1% 364/29688 [05:36<7:28:02,  1.09it/s]  1% 365/29688 [05:37<7:28:31,  1.09it/s]  1% 366/29688 [05:38<7:27:32,  1.09it/s]  1% 367/29688 [05:39<7:26:56,  1.09it/s]  1% 368/29688 [05:40<7:26:27,  1.09it/s]  1% 369/29688 [05:41<7:27:11,  1.09it/s]  1% 370/29688 [05:42<7:26:42,  1.09it/s]  1% 371/29688 [05:42<7:26:00,  1.10it/s]  1% 372/29688 [05:43<7:25:30,  1.10it/s]  1% 373/29688 [05:44<7:29:14,  1.09it/s]  1% 374/29688 [05:45<7:27:36,  1.09it/s]  1% 375/29688 [05:46<7:26:31,  1.09it/s]  1% 376/29688 [05:47<7:25:27,  1.10it/s]  1% 377/29688 [05:48<7:26:10,  1.09it/s]  1% 378/29688 [05:49<7:24:55,  1.10it/s]  1% 379/29688 [05:50<7:24:39,  1.10it/s]  1% 380/29688 [05:51<7:24:54,  1.10it/s]  1% 381/29688 [05:52<7:24:57,  1.10it/s]  1% 382/29688 [05:52<7:24:53,  1.10it/s]  1% 383/29688 [05:53<7:25:16,  1.10it/s]  1% 384/29688 [05:54<7:25:03,  1.10it/s]  1% 385/29688 [05:55<7:24:38,  1.10it/s]  1% 386/29688 [05:56<7:25:27,  1.10it/s]  1% 387/29688 [05:57<7:25:24,  1.10it/s]  1% 388/29688 [05:58<7:25:06,  1.10it/s]  1% 389/29688 [05:59<7:25:10,  1.10it/s]  1% 390/29688 [06:00<7:24:11,  1.10it/s]  1% 391/29688 [06:01<7:24:40,  1.10it/s]  1% 392/29688 [06:02<7:26:33,  1.09it/s]  1% 393/29688 [06:03<7:26:58,  1.09it/s]  1% 394/29688 [06:03<7:25:09,  1.10it/s]  1% 395/29688 [06:04<7:25:52,  1.09it/s]  1% 396/29688 [06:05<7:25:26,  1.10it/s]  1% 397/29688 [06:06<7:24:48,  1.10it/s]  1% 398/29688 [06:07<7:24:11,  1.10it/s]  1% 399/29688 [06:08<7:24:39,  1.10it/s]  1% 400/29688 [06:09<7:24:54,  1.10it/s]  1% 401/29688 [06:10<7:25:37,  1.10it/s]  1% 402/29688 [06:11<7:24:14,  1.10it/s]  1% 403/29688 [06:12<7:24:16,  1.10it/s]  1% 404/29688 [06:14<10:06:05,  1.24s/it]  1% 405/29688 [06:15<9:17:19,  1.14s/it]   1% 406/29688 [06:15<8:43:37,  1.07s/it]  1% 407/29688 [06:16<8:19:52,  1.02s/it]  1% 408/29688 [06:17<8:03:26,  1.01it/s]  1% 409/29688 [06:18<7:51:37,  1.03it/s]  1% 410/29688 [06:19<7:43:12,  1.05it/s]  1% 411/29688 [06:20<7:37:30,  1.07it/s]  1% 412/29688 [06:21<7:35:05,  1.07it/s]  1% 413/29688 [06:22<7:32:48,  1.08it/s]  1% 414/29688 [06:23<7:31:30,  1.08it/s]  1% 415/29688 [06:24<7:29:39,  1.08it/s]  1% 416/29688 [06:25<7:29:01,  1.09it/s]  1% 417/29688 [06:26<7:27:07,  1.09it/s]  1% 418/29688 [06:26<7:26:36,  1.09it/s]  1% 419/29688 [06:27<7:25:37,  1.09it/s]  1% 420/29688 [06:28<7:25:18,  1.10it/s]  1% 421/29688 [06:29<7:24:44,  1.10it/s]  1% 422/29688 [06:30<7:23:37,  1.10it/s]  1% 423/29688 [06:31<7:23:20,  1.10it/s]  1% 424/29688 [06:32<7:22:28,  1.10it/s]  1% 425/29688 [06:33<7:22:26,  1.10it/s]  1% 426/29688 [06:34<7:22:19,  1.10it/s]  1% 427/29688 [06:35<7:21:41,  1.10it/s]  1% 428/29688 [06:36<7:23:24,  1.10it/s]  1% 429/29688 [06:36<7:23:28,  1.10it/s]  1% 430/29688 [06:37<7:23:43,  1.10it/s]  1% 431/29688 [06:38<7:23:35,  1.10it/s]  1% 432/29688 [06:39<7:23:49,  1.10it/s]  1% 433/29688 [06:40<7:24:15,  1.10it/s]  1% 434/29688 [06:41<7:23:32,  1.10it/s]  1% 435/29688 [06:42<7:23:21,  1.10it/s]  1% 436/29688 [06:43<7:23:14,  1.10it/s]  1% 437/29688 [06:44<7:22:14,  1.10it/s]  1% 438/29688 [06:45<7:24:05,  1.10it/s]  1% 439/29688 [06:46<7:22:35,  1.10it/s]  1% 440/29688 [06:46<7:22:44,  1.10it/s]  1% 441/29688 [06:47<7:23:10,  1.10it/s]  1% 442/29688 [06:48<7:22:27,  1.10it/s]  1% 443/29688 [06:49<7:23:49,  1.10it/s]  1% 444/29688 [06:50<7:23:15,  1.10it/s]  1% 445/29688 [06:51<7:22:01,  1.10it/s]  2% 446/29688 [06:52<7:22:21,  1.10it/s]  2% 447/29688 [06:53<7:22:41,  1.10it/s]  2% 448/29688 [06:54<7:22:35,  1.10it/s]  2% 449/29688 [06:55<7:21:14,  1.10it/s]  2% 450/29688 [06:55<7:21:10,  1.10it/s]  2% 451/29688 [06:56<7:21:12,  1.10it/s]  2% 452/29688 [06:57<7:21:00,  1.10it/s]  2% 453/29688 [06:58<7:21:12,  1.10it/s]  2% 454/29688 [06:59<7:22:06,  1.10it/s]  2% 455/29688 [07:00<7:22:38,  1.10it/s]  2% 456/29688 [07:01<7:22:30,  1.10it/s]  2% 457/29688 [07:02<7:23:21,  1.10it/s]  2% 458/29688 [07:03<7:24:06,  1.10it/s]  2% 459/29688 [07:04<7:24:20,  1.10it/s]  2% 460/29688 [07:05<7:23:26,  1.10it/s]  2% 461/29688 [07:05<7:23:01,  1.10it/s]  2% 462/29688 [07:06<7:24:27,  1.10it/s]  2% 463/29688 [07:07<7:24:41,  1.10it/s]  2% 464/29688 [07:08<7:24:21,  1.10it/s]  2% 465/29688 [07:09<7:23:00,  1.10it/s]  2% 466/29688 [07:10<7:22:36,  1.10it/s]  2% 467/29688 [07:11<7:22:54,  1.10it/s]  2% 468/29688 [07:12<7:23:11,  1.10it/s]  2% 469/29688 [07:13<7:22:56,  1.10it/s]  2% 470/29688 [07:14<7:21:52,  1.10it/s]  2% 471/29688 [07:15<7:21:50,  1.10it/s]  2% 472/29688 [07:16<7:21:50,  1.10it/s]  2% 473/29688 [07:16<7:21:43,  1.10it/s]  2% 474/29688 [07:17<7:22:05,  1.10it/s]  2% 475/29688 [07:18<7:22:17,  1.10it/s]  2% 476/29688 [07:19<7:22:43,  1.10it/s]  2% 477/29688 [07:20<7:22:11,  1.10it/s]  2% 478/29688 [07:21<7:22:45,  1.10it/s]  2% 479/29688 [07:22<7:22:33,  1.10it/s]  2% 480/29688 [07:23<7:22:17,  1.10it/s]  2% 481/29688 [07:24<7:22:50,  1.10it/s]  2% 482/29688 [07:25<7:23:38,  1.10it/s]  2% 483/29688 [07:26<7:23:59,  1.10it/s]  2% 484/29688 [07:26<7:23:02,  1.10it/s]  2% 485/29688 [07:27<7:23:24,  1.10it/s]  2% 486/29688 [07:28<7:24:48,  1.09it/s]  2% 487/29688 [07:29<7:24:05,  1.10it/s]  2% 488/29688 [07:30<7:24:05,  1.10it/s]  2% 489/29688 [07:31<7:25:05,  1.09it/s]  2% 490/29688 [07:32<7:24:33,  1.09it/s]  2% 491/29688 [07:33<7:25:06,  1.09it/s]  2% 492/29688 [07:34<7:24:26,  1.09it/s]  2% 493/29688 [07:35<7:23:52,  1.10it/s]  2% 494/29688 [07:36<7:23:41,  1.10it/s]  2% 495/29688 [07:36<7:24:31,  1.09it/s]  2% 496/29688 [07:37<7:24:07,  1.10it/s]  2% 497/29688 [07:38<7:23:38,  1.10it/s]  2% 498/29688 [07:39<7:23:57,  1.10it/s]  2% 499/29688 [07:40<7:23:31,  1.10it/s]  2% 500/29688 [07:41<8:12:24,  1.01s/it]  2% 501/29688 [07:42<8:06:15,  1.00it/s]  2% 502/29688 [07:43<7:53:27,  1.03it/s]  2% 503/29688 [07:44<7:46:56,  1.04it/s]  2% 504/29688 [07:45<7:39:18,  1.06it/s]  2% 505/29688 [07:46<7:34:04,  1.07it/s]  2% 506/29688 [07:47<7:30:09,  1.08it/s]  2% 507/29688 [07:48<7:29:01,  1.08it/s]  2% 508/29688 [07:49<7:28:30,  1.08it/s]  2% 509/29688 [07:50<7:26:41,  1.09it/s]  2% 510/29688 [07:51<7:26:43,  1.09it/s]  2% 511/29688 [07:51<7:25:41,  1.09it/s]  2% 512/29688 [07:52<7:25:53,  1.09it/s]  2% 513/29688 [07:53<7:25:07,  1.09it/s]  2% 514/29688 [07:54<7:25:25,  1.09it/s]  2% 515/29688 [07:55<7:24:54,  1.09it/s]  2% 516/29688 [07:56<7:25:28,  1.09it/s]  2% 517/29688 [07:57<7:24:38,  1.09it/s]  2% 518/29688 [07:58<7:25:17,  1.09it/s]  2% 519/29688 [07:59<7:25:41,  1.09it/s]  2% 520/29688 [08:01<10:06:29,  1.25s/it]  2% 521/29688 [08:02<9:17:46,  1.15s/it]   2% 522/29688 [08:03<8:44:14,  1.08s/it]  2% 523/29688 [08:04<8:21:14,  1.03s/it]  2% 524/29688 [08:04<8:05:07,  1.00it/s]  2% 525/29688 [08:05<7:51:36,  1.03it/s]  2% 526/29688 [08:06<7:42:23,  1.05it/s]  2% 527/29688 [08:07<7:37:31,  1.06it/s]  2% 528/29688 [08:08<7:32:36,  1.07it/s]  2% 529/29688 [08:09<7:29:47,  1.08it/s]  2% 530/29688 [08:10<7:27:24,  1.09it/s]  2% 531/29688 [08:11<7:25:59,  1.09it/s]  2% 532/29688 [08:12<7:24:33,  1.09it/s]  2% 533/29688 [08:13<7:23:31,  1.10it/s]  2% 534/29688 [08:14<7:23:16,  1.10it/s]  2% 535/29688 [08:15<7:22:50,  1.10it/s]  2% 536/29688 [08:15<7:22:29,  1.10it/s]  2% 537/29688 [08:16<7:22:30,  1.10it/s]  2% 538/29688 [08:17<7:22:52,  1.10it/s]  2% 539/29688 [08:18<7:22:08,  1.10it/s]  2% 540/29688 [08:19<7:22:09,  1.10it/s]  2% 541/29688 [08:20<7:21:15,  1.10it/s]  2% 542/29688 [08:21<7:22:09,  1.10it/s]  2% 543/29688 [08:22<7:22:20,  1.10it/s]  2% 544/29688 [08:23<7:22:43,  1.10it/s]  2% 545/29688 [08:24<7:22:09,  1.10it/s]  2% 546/29688 [08:25<7:22:13,  1.10it/s]  2% 547/29688 [08:25<7:23:10,  1.10it/s]  2% 548/29688 [08:26<7:22:21,  1.10it/s]  2% 549/29688 [08:27<7:22:50,  1.10it/s]  2% 550/29688 [08:28<7:23:41,  1.09it/s]  2% 551/29688 [08:29<7:23:39,  1.09it/s]  2% 552/29688 [08:30<7:23:07,  1.10it/s]  2% 553/29688 [08:31<7:22:56,  1.10it/s]  2% 554/29688 [08:32<7:23:37,  1.09it/s]  2% 555/29688 [08:33<7:23:19,  1.10it/s]  2% 556/29688 [08:34<7:22:23,  1.10it/s]  2% 557/29688 [08:35<7:22:36,  1.10it/s]  2% 558/29688 [08:35<7:23:31,  1.09it/s]  2% 559/29688 [08:36<7:24:03,  1.09it/s]  2% 560/29688 [08:37<7:23:28,  1.09it/s]  2% 561/29688 [08:38<7:22:18,  1.10it/s]  2% 562/29688 [08:39<7:21:31,  1.10it/s]  2% 563/29688 [08:40<7:21:08,  1.10it/s]  2% 564/29688 [08:41<7:22:05,  1.10it/s]  2% 565/29688 [08:42<7:23:02,  1.10it/s]  2% 566/29688 [08:43<7:21:51,  1.10it/s]  2% 567/29688 [08:44<7:22:14,  1.10it/s]  2% 568/29688 [08:45<7:24:08,  1.09it/s]  2% 569/29688 [08:45<7:22:34,  1.10it/s]  2% 570/29688 [08:46<7:21:32,  1.10it/s]  2% 571/29688 [08:47<7:21:04,  1.10it/s]  2% 572/29688 [08:48<7:23:17,  1.09it/s]  2% 573/29688 [08:49<7:21:25,  1.10it/s]  2% 574/29688 [08:50<7:20:44,  1.10it/s]  2% 575/29688 [08:51<7:20:54,  1.10it/s]  2% 576/29688 [08:52<7:20:25,  1.10it/s]  2% 577/29688 [08:53<7:20:07,  1.10it/s]  2% 578/29688 [08:54<7:20:17,  1.10it/s]  2% 579/29688 [08:55<7:20:12,  1.10it/s]  2% 580/29688 [08:55<7:20:48,  1.10it/s]  2% 581/29688 [08:56<7:19:51,  1.10it/s]  2% 582/29688 [08:57<7:20:36,  1.10it/s]  2% 583/29688 [08:58<7:20:41,  1.10it/s]  2% 584/29688 [08:59<7:21:13,  1.10it/s]  2% 585/29688 [09:00<7:22:08,  1.10it/s]  2% 586/29688 [09:01<7:22:40,  1.10it/s]  2% 587/29688 [09:02<7:22:16,  1.10it/s]  2% 588/29688 [09:03<7:22:01,  1.10it/s]  2% 589/29688 [09:04<7:22:22,  1.10it/s]  2% 590/29688 [09:05<7:22:30,  1.10it/s]  2% 591/29688 [09:06<7:21:12,  1.10it/s]  2% 592/29688 [09:06<7:20:30,  1.10it/s]  2% 593/29688 [09:07<7:21:05,  1.10it/s]  2% 594/29688 [09:08<7:20:37,  1.10it/s]  2% 595/29688 [09:09<7:22:10,  1.10it/s]  2% 596/29688 [09:10<7:22:26,  1.10it/s]  2% 597/29688 [09:11<7:22:06,  1.10it/s]  2% 598/29688 [09:12<7:21:35,  1.10it/s]  2% 599/29688 [09:13<7:21:23,  1.10it/s]  2% 600/29688 [09:14<7:20:50,  1.10it/s]  2% 601/29688 [09:15<7:20:02,  1.10it/s]  2% 602/29688 [09:16<7:20:16,  1.10it/s]  2% 603/29688 [09:16<7:22:16,  1.10it/s]  2% 604/29688 [09:17<7:21:43,  1.10it/s]  2% 605/29688 [09:18<7:20:14,  1.10it/s]  2% 606/29688 [09:19<7:20:22,  1.10it/s]  2% 607/29688 [09:20<7:20:14,  1.10it/s]  2% 608/29688 [09:21<7:20:41,  1.10it/s]  2% 609/29688 [09:22<7:21:17,  1.10it/s]  2% 610/29688 [09:23<7:19:47,  1.10it/s]  2% 611/29688 [09:24<7:19:46,  1.10it/s]  2% 612/29688 [09:25<7:19:59,  1.10it/s]  2% 613/29688 [09:26<7:19:18,  1.10it/s]  2% 614/29688 [09:26<7:18:55,  1.10it/s]  2% 615/29688 [09:28<10:00:02,  1.24s/it]  2% 616/29688 [09:29<9:13:26,  1.14s/it]   2% 617/29688 [09:30<8:40:19,  1.07s/it]  2% 618/29688 [09:31<8:16:34,  1.02s/it]  2% 619/29688 [09:32<8:00:07,  1.01it/s]  2% 620/29688 [09:33<7:49:29,  1.03it/s]  2% 621/29688 [09:34<7:40:35,  1.05it/s]  2% 622/29688 [09:35<7:34:47,  1.07it/s]  2% 623/29688 [09:36<7:30:32,  1.08it/s]  2% 624/29688 [09:37<7:28:55,  1.08it/s]  2% 625/29688 [09:38<7:25:34,  1.09it/s]  2% 626/29688 [09:38<7:23:14,  1.09it/s]  2% 627/29688 [09:39<7:23:02,  1.09it/s]  2% 628/29688 [09:40<7:24:13,  1.09it/s]  2% 629/29688 [09:41<7:22:56,  1.09it/s]  2% 630/29688 [09:42<7:21:52,  1.10it/s]  2% 631/29688 [09:43<7:22:27,  1.09it/s]  2% 632/29688 [09:44<7:21:47,  1.10it/s]  2% 633/29688 [09:45<7:23:21,  1.09it/s]  2% 634/29688 [09:46<7:22:36,  1.09it/s]  2% 635/29688 [09:47<7:24:01,  1.09it/s]  2% 636/29688 [09:48<7:22:25,  1.09it/s]  2% 637/29688 [09:49<7:21:45,  1.10it/s]  2% 638/29688 [09:49<7:22:59,  1.09it/s]  2% 639/29688 [09:50<7:22:16,  1.09it/s]  2% 640/29688 [09:51<7:21:51,  1.10it/s]  2% 641/29688 [09:52<7:19:47,  1.10it/s]  2% 642/29688 [09:53<7:20:12,  1.10it/s]  2% 643/29688 [09:54<7:19:48,  1.10it/s]  2% 644/29688 [09:55<7:19:59,  1.10it/s]  2% 645/29688 [09:56<7:20:21,  1.10it/s]  2% 646/29688 [09:57<7:20:25,  1.10it/s]  2% 647/29688 [09:58<7:20:00,  1.10it/s]  2% 648/29688 [09:59<7:19:21,  1.10it/s]  2% 649/29688 [09:59<7:19:27,  1.10it/s]  2% 650/29688 [10:00<7:18:43,  1.10it/s]  2% 651/29688 [10:01<7:18:41,  1.10it/s]  2% 652/29688 [10:02<7:19:19,  1.10it/s]  2% 653/29688 [10:03<7:19:25,  1.10it/s]  2% 654/29688 [10:04<7:19:39,  1.10it/s]  2% 655/29688 [10:05<7:19:53,  1.10it/s]  2% 656/29688 [10:06<7:20:06,  1.10it/s]  2% 657/29688 [10:07<7:20:20,  1.10it/s]  2% 658/29688 [10:08<7:19:26,  1.10it/s]  2% 659/29688 [10:08<7:18:47,  1.10it/s]  2% 660/29688 [10:09<7:19:11,  1.10it/s]  2% 661/29688 [10:10<7:20:05,  1.10it/s]  2% 662/29688 [10:11<7:21:51,  1.09it/s]  2% 663/29688 [10:12<7:22:08,  1.09it/s]  2% 664/29688 [10:13<7:22:55,  1.09it/s]  2% 665/29688 [10:14<7:22:19,  1.09it/s]  2% 666/29688 [10:15<7:23:01,  1.09it/s]  2% 667/29688 [10:16<7:22:27,  1.09it/s]  2% 668/29688 [10:17<7:22:51,  1.09it/s]  2% 669/29688 [10:18<7:23:11,  1.09it/s]  2% 670/29688 [10:19<7:22:32,  1.09it/s]  2% 671/29688 [10:19<7:22:59,  1.09it/s]  2% 672/29688 [10:20<7:23:38,  1.09it/s]  2% 673/29688 [10:21<7:24:00,  1.09it/s]  2% 674/29688 [10:22<7:23:24,  1.09it/s]  2% 675/29688 [10:23<7:23:25,  1.09it/s]  2% 676/29688 [10:24<7:23:42,  1.09it/s]  2% 677/29688 [10:25<7:23:52,  1.09it/s]  2% 678/29688 [10:26<7:23:12,  1.09it/s]  2% 679/29688 [10:27<7:23:27,  1.09it/s]  2% 680/29688 [10:28<7:22:34,  1.09it/s]  2% 681/29688 [10:29<7:22:53,  1.09it/s]  2% 682/29688 [10:30<7:22:55,  1.09it/s]  2% 683/29688 [10:30<7:22:23,  1.09it/s]  2% 684/29688 [10:31<7:22:06,  1.09it/s]  2% 685/29688 [10:32<7:22:16,  1.09it/s]  2% 686/29688 [10:33<7:22:57,  1.09it/s]  2% 687/29688 [10:34<7:22:35,  1.09it/s]  2% 688/29688 [10:35<7:22:58,  1.09it/s]  2% 689/29688 [10:36<7:22:29,  1.09it/s]  2% 690/29688 [10:37<7:22:49,  1.09it/s]  2% 691/29688 [10:38<7:22:09,  1.09it/s]  2% 692/29688 [10:39<7:22:23,  1.09it/s]  2% 693/29688 [10:40<7:22:52,  1.09it/s]  2% 694/29688 [10:41<7:22:28,  1.09it/s]  2% 695/29688 [10:43<10:00:55,  1.24s/it]  2% 696/29688 [10:43<9:15:14,  1.15s/it]   2% 697/29688 [10:44<8:40:06,  1.08s/it]  2% 698/29688 [10:45<8:16:47,  1.03s/it]  2% 699/29688 [10:46<7:59:32,  1.01it/s]  2% 700/29688 [10:47<7:47:35,  1.03it/s]  2% 701/29688 [10:48<7:39:03,  1.05it/s]  2% 702/29688 [10:49<7:34:59,  1.06it/s]  2% 703/29688 [10:50<7:31:21,  1.07it/s]  2% 704/29688 [10:51<7:27:11,  1.08it/s]  2% 705/29688 [10:52<7:25:44,  1.08it/s]  2% 706/29688 [10:53<7:25:09,  1.09it/s]  2% 707/29688 [10:54<7:23:51,  1.09it/s]  2% 708/29688 [10:54<7:23:22,  1.09it/s]  2% 709/29688 [10:55<7:22:36,  1.09it/s]  2% 710/29688 [10:56<7:21:29,  1.09it/s]  2% 711/29688 [10:57<7:22:04,  1.09it/s]  2% 712/29688 [10:58<7:21:32,  1.09it/s]  2% 713/29688 [10:59<7:20:58,  1.10it/s]  2% 714/29688 [11:00<7:21:55,  1.09it/s]  2% 715/29688 [11:01<7:21:12,  1.09it/s]  2% 716/29688 [11:02<7:21:53,  1.09it/s]  2% 717/29688 [11:03<7:20:10,  1.10it/s]  2% 718/29688 [11:04<7:20:45,  1.10it/s]  2% 719/29688 [11:05<7:21:57,  1.09it/s]  2% 720/29688 [11:05<7:23:20,  1.09it/s]  2% 721/29688 [11:06<7:22:41,  1.09it/s]  2% 722/29688 [11:07<7:21:43,  1.09it/s]  2% 723/29688 [11:08<7:22:00,  1.09it/s]  2% 724/29688 [11:09<7:21:33,  1.09it/s]  2% 725/29688 [11:10<7:21:27,  1.09it/s]  2% 726/29688 [11:11<7:21:17,  1.09it/s]  2% 727/29688 [11:12<7:21:53,  1.09it/s]  2% 728/29688 [11:13<7:22:07,  1.09it/s]  2% 729/29688 [11:14<7:22:23,  1.09it/s]  2% 730/29688 [11:15<7:21:32,  1.09it/s]  2% 731/29688 [11:15<7:20:45,  1.09it/s]  2% 732/29688 [11:16<7:20:17,  1.10it/s]  2% 733/29688 [11:17<7:20:34,  1.10it/s]  2% 734/29688 [11:18<7:22:15,  1.09it/s]  2% 735/29688 [11:19<7:21:38,  1.09it/s]  2% 736/29688 [11:20<7:20:44,  1.09it/s]  2% 737/29688 [11:21<7:20:18,  1.10it/s]  2% 738/29688 [11:22<7:20:32,  1.10it/s]  2% 739/29688 [11:23<7:20:46,  1.09it/s]  2% 740/29688 [11:24<7:20:11,  1.10it/s]  2% 741/29688 [11:25<7:20:12,  1.10it/s]  2% 742/29688 [11:26<7:20:48,  1.09it/s]  3% 743/29688 [11:26<7:20:03,  1.10it/s]  3% 744/29688 [11:27<7:19:50,  1.10it/s]  3% 745/29688 [11:28<7:19:15,  1.10it/s]  3% 746/29688 [11:29<7:19:27,  1.10it/s]  3% 747/29688 [11:30<7:19:28,  1.10it/s]  3% 748/29688 [11:31<7:18:58,  1.10it/s]  3% 749/29688 [11:32<7:20:24,  1.10it/s]  3% 750/29688 [11:33<7:20:01,  1.10it/s]  3% 751/29688 [11:34<7:18:58,  1.10it/s]  3% 752/29688 [11:35<7:17:38,  1.10it/s]  3% 753/29688 [11:36<7:17:46,  1.10it/s]  3% 754/29688 [11:36<7:18:41,  1.10it/s]  3% 755/29688 [11:37<7:18:47,  1.10it/s]  3% 756/29688 [11:38<7:18:57,  1.10it/s]  3% 757/29688 [11:39<7:17:37,  1.10it/s]  3% 758/29688 [11:40<7:17:53,  1.10it/s]  3% 759/29688 [11:41<7:19:30,  1.10it/s]  3% 760/29688 [11:42<7:17:58,  1.10it/s]  3% 761/29688 [11:43<7:17:10,  1.10it/s]  3% 762/29688 [11:44<7:16:31,  1.10it/s]  3% 763/29688 [11:45<7:19:09,  1.10it/s]  3% 764/29688 [11:46<7:18:15,  1.10it/s]  3% 765/29688 [11:46<7:18:54,  1.10it/s]  3% 766/29688 [11:47<7:17:38,  1.10it/s]  3% 767/29688 [11:48<7:19:14,  1.10it/s]  3% 768/29688 [11:49<7:18:50,  1.10it/s]  3% 769/29688 [11:50<7:17:32,  1.10it/s]  3% 770/29688 [11:51<7:17:47,  1.10it/s]  3% 771/29688 [11:52<7:16:42,  1.10it/s]  3% 772/29688 [11:53<7:16:39,  1.10it/s]  3% 773/29688 [11:54<7:17:11,  1.10it/s]  3% 774/29688 [11:55<7:16:53,  1.10it/s]  3% 775/29688 [11:56<7:18:13,  1.10it/s]  3% 776/29688 [11:56<7:20:07,  1.09it/s]  3% 777/29688 [11:57<7:19:20,  1.10it/s]  3% 778/29688 [11:58<7:18:47,  1.10it/s]  3% 779/29688 [11:59<7:17:58,  1.10it/s]  3% 780/29688 [12:00<7:17:30,  1.10it/s]  3% 781/29688 [12:01<7:17:57,  1.10it/s]  3% 782/29688 [12:02<7:16:54,  1.10it/s]  3% 783/29688 [12:03<7:16:45,  1.10it/s]  3% 784/29688 [12:04<7:15:56,  1.11it/s]  3% 785/29688 [12:05<7:17:06,  1.10it/s]  3% 786/29688 [12:06<7:16:20,  1.10it/s]  3% 787/29688 [12:06<7:16:30,  1.10it/s]  3% 788/29688 [12:07<7:16:15,  1.10it/s]  3% 789/29688 [12:08<7:16:14,  1.10it/s]  3% 790/29688 [12:09<7:18:09,  1.10it/s]  3% 791/29688 [12:10<7:18:30,  1.10it/s]  3% 792/29688 [12:11<7:19:32,  1.10it/s]  3% 793/29688 [12:12<7:19:21,  1.10it/s]  3% 794/29688 [12:13<7:19:56,  1.09it/s]  3% 795/29688 [12:14<7:19:46,  1.09it/s]  3% 796/29688 [12:15<7:19:21,  1.10it/s]  3% 797/29688 [12:16<7:18:58,  1.10it/s]  3% 798/29688 [12:16<7:18:43,  1.10it/s]  3% 799/29688 [12:17<7:18:42,  1.10it/s]  3% 800/29688 [12:18<7:19:23,  1.10it/s]  3% 801/29688 [12:19<7:18:37,  1.10it/s]  3% 802/29688 [12:20<7:18:30,  1.10it/s]  3% 803/29688 [12:21<7:19:51,  1.09it/s]  3% 804/29688 [12:22<7:20:00,  1.09it/s]  3% 805/29688 [12:23<7:19:18,  1.10it/s]  3% 806/29688 [12:24<7:20:36,  1.09it/s]  3% 807/29688 [12:25<7:20:43,  1.09it/s]  3% 808/29688 [12:26<7:20:32,  1.09it/s]  3% 809/29688 [12:27<7:20:43,  1.09it/s]  3% 810/29688 [12:27<7:20:30,  1.09it/s]  3% 811/29688 [12:29<10:02:38,  1.25s/it]  3% 812/29688 [12:30<9:14:30,  1.15s/it]   3% 813/29688 [12:31<8:39:28,  1.08s/it]  3% 814/29688 [12:32<8:14:57,  1.03s/it]  3% 815/29688 [12:33<7:59:14,  1.00it/s]  3% 816/29688 [12:34<7:46:35,  1.03it/s]  3% 817/29688 [12:35<7:37:23,  1.05it/s]  3% 818/29688 [12:36<7:32:20,  1.06it/s]  3% 819/29688 [12:37<7:29:17,  1.07it/s]  3% 820/29688 [12:38<7:25:48,  1.08it/s]  3% 821/29688 [12:39<7:22:24,  1.09it/s]  3% 822/29688 [12:39<7:20:52,  1.09it/s]  3% 823/29688 [12:40<7:20:06,  1.09it/s]  3% 824/29688 [12:41<7:18:44,  1.10it/s]  3% 825/29688 [12:42<7:18:47,  1.10it/s]  3% 826/29688 [12:43<7:18:10,  1.10it/s]  3% 827/29688 [12:44<7:17:04,  1.10it/s]  3% 828/29688 [12:45<7:18:14,  1.10it/s]  3% 829/29688 [12:46<7:18:17,  1.10it/s]  3% 830/29688 [12:47<7:17:01,  1.10it/s]  3% 831/29688 [12:48<7:16:55,  1.10it/s]  3% 832/29688 [12:49<7:15:58,  1.10it/s]  3% 833/29688 [12:49<7:17:47,  1.10it/s]  3% 834/29688 [12:50<7:17:01,  1.10it/s]  3% 835/29688 [12:51<7:18:09,  1.10it/s]  3% 836/29688 [12:52<7:19:03,  1.10it/s]  3% 837/29688 [12:53<7:19:06,  1.10it/s]  3% 838/29688 [12:54<7:19:32,  1.09it/s]  3% 839/29688 [12:55<7:19:08,  1.09it/s]  3% 840/29688 [12:56<7:19:53,  1.09it/s]  3% 841/29688 [12:57<7:20:30,  1.09it/s]  3% 842/29688 [12:58<7:21:00,  1.09it/s]  3% 843/29688 [12:59<7:20:20,  1.09it/s]  3% 844/29688 [13:00<7:21:10,  1.09it/s]  3% 845/29688 [13:00<7:21:09,  1.09it/s]  3% 846/29688 [13:01<7:21:17,  1.09it/s]  3% 847/29688 [13:02<7:20:05,  1.09it/s]  3% 848/29688 [13:03<7:19:28,  1.09it/s]  3% 849/29688 [13:04<7:18:50,  1.10it/s]  3% 850/29688 [13:05<7:19:39,  1.09it/s]  3% 851/29688 [13:06<7:19:15,  1.09it/s]  3% 852/29688 [13:07<7:17:54,  1.10it/s]  3% 853/29688 [13:08<7:18:07,  1.10it/s]  3% 854/29688 [13:09<7:18:06,  1.10it/s]  3% 855/29688 [13:10<7:19:16,  1.09it/s]  3% 856/29688 [13:11<7:17:53,  1.10it/s]  3% 857/29688 [13:11<7:18:02,  1.10it/s]  3% 858/29688 [13:12<7:18:05,  1.10it/s]  3% 859/29688 [13:13<7:17:57,  1.10it/s]  3% 860/29688 [13:14<7:18:11,  1.10it/s]  3% 861/29688 [13:15<7:18:37,  1.10it/s]  3% 862/29688 [13:16<7:18:22,  1.10it/s]  3% 863/29688 [13:17<7:17:55,  1.10it/s]  3% 864/29688 [13:18<7:17:28,  1.10it/s]  3% 865/29688 [13:19<7:16:00,  1.10it/s]  3% 866/29688 [13:20<7:16:13,  1.10it/s]  3% 867/29688 [13:21<7:16:21,  1.10it/s]  3% 868/29688 [13:21<7:15:33,  1.10it/s]  3% 869/29688 [13:22<7:16:55,  1.10it/s]  3% 870/29688 [13:23<7:15:41,  1.10it/s]  3% 871/29688 [13:24<7:14:55,  1.10it/s]  3% 872/29688 [13:25<7:16:16,  1.10it/s]  3% 873/29688 [13:26<7:15:49,  1.10it/s]  3% 874/29688 [13:27<7:14:46,  1.10it/s]  3% 875/29688 [13:28<7:15:06,  1.10it/s]  3% 876/29688 [13:29<7:14:27,  1.11it/s]  3% 877/29688 [13:30<7:16:18,  1.10it/s]  3% 878/29688 [13:31<7:16:32,  1.10it/s]  3% 879/29688 [13:31<7:16:53,  1.10it/s]  3% 880/29688 [13:32<7:16:50,  1.10it/s]  3% 881/29688 [13:33<7:16:42,  1.10it/s]  3% 882/29688 [13:34<7:15:29,  1.10it/s]  3% 883/29688 [13:35<7:15:33,  1.10it/s]  3% 884/29688 [13:36<7:15:00,  1.10it/s]  3% 885/29688 [13:37<7:16:17,  1.10it/s]  3% 886/29688 [13:38<7:16:36,  1.10it/s]  3% 887/29688 [13:39<7:16:56,  1.10it/s]  3% 888/29688 [13:40<7:16:20,  1.10it/s]  3% 889/29688 [13:41<7:15:56,  1.10it/s]  3% 890/29688 [13:41<7:15:39,  1.10it/s]  3% 891/29688 [13:42<7:15:44,  1.10it/s]  3% 892/29688 [13:43<7:15:35,  1.10it/s]  3% 893/29688 [13:44<7:16:58,  1.10it/s]  3% 894/29688 [13:45<7:16:37,  1.10it/s]  3% 895/29688 [13:46<7:16:21,  1.10it/s]  3% 896/29688 [13:47<7:15:26,  1.10it/s]  3% 897/29688 [13:48<7:17:02,  1.10it/s]  3% 898/29688 [13:49<7:16:53,  1.10it/s]  3% 899/29688 [13:50<7:16:48,  1.10it/s]  3% 900/29688 [13:51<7:16:05,  1.10it/s]  3% 901/29688 [13:51<7:15:14,  1.10it/s]  3% 902/29688 [13:52<7:14:33,  1.10it/s]  3% 903/29688 [13:53<7:15:21,  1.10it/s]  3% 904/29688 [13:54<7:15:04,  1.10it/s]  3% 905/29688 [13:55<7:14:36,  1.10it/s]  3% 906/29688 [13:57<9:51:55,  1.23s/it]  3% 907/29688 [13:58<9:05:23,  1.14s/it]  3% 908/29688 [13:59<8:30:46,  1.06s/it]  3% 909/29688 [14:00<8:08:59,  1.02s/it]  3% 910/29688 [14:01<7:52:37,  1.01it/s]  3% 911/29688 [14:02<7:41:00,  1.04it/s]  3% 912/29688 [14:02<7:33:14,  1.06it/s]  3% 913/29688 [14:03<7:27:08,  1.07it/s]  3% 914/29688 [14:04<7:23:45,  1.08it/s]Traceback (most recent call last):
  File "SCAL_transformer.py", line 472, in <module>
    model = run_training(model)
  File "SCAL_transformer.py", line 340, in run_training
    trained_model = load_and_train(model, amount_per_it, iteration)
  File "SCAL_transformer.py", line 291, in load_and_train
    train(model, train_dataset, val_dataset)
  File "SCAL_transformer.py", line 237, in train
    trainer.train()
  File "/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/transformers/trainer.py", line 1413, in train
    ignore_keys_for_eval=ignore_keys_for_eval,
  File "/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/transformers/trainer.py", line 1651, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/transformers/trainer.py", line 2345, in training_step
    loss = self.compute_loss(model, inputs)
  File "/cluster/home/mezhang/twitter-sent-analysis/trainers/myScalTrainer.py", line 12, in compute_loss
    outputs = model(**inputs, return_dict = True)
  File "/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/cluster/home/mezhang/twitter-sent-analysis/models/myScalModel.py", line 143, in forward
    return_dict=return_dict,
  File "/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py", line 505, in forward
    output_attentions=output_attentions,
  File "/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py", line 353, in forward
    intermediate_output = self.intermediate(attention_output)
  File "/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py", line 305, in forward
    hidden_states = self.dense(hidden_states)
  File "/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
  File "/cluster/home/mezhang/miniconda3/envs/cil_env/lib/python3.7/traceback.py", line 193, in format_stack
    def format_stack(f=None, limit=None):
KeyboardInterrupt
wandb: Waiting for W&B process to finish... (failed 255). Press Control-C to abort syncing.
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.442 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.449 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.449 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.449 MB uploaded (0.000 MB deduped)wandb: / 0.449 MB of 0.449 MB uploaded (0.000 MB deduped)wandb: - 0.449 MB of 0.449 MB uploaded (0.000 MB deduped)wandb: \ 0.449 MB of 0.449 MB uploaded (0.000 MB deduped)wandb: | 0.449 MB of 0.449 MB uploaded (0.000 MB deduped)wandb: / 0.449 MB of 0.449 MB uploaded (0.000 MB deduped)wandb: - 0.449 MB of 0.449 MB uploaded (0.000 MB deduped)wandb: \ 0.449 MB of 0.449 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: Synced devoted-music-10: https://wandb.ai/mezhang/twitter-sentiment-analysis-scal/runs/1x8y0jxo
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220723_202537-1x8y0jxo/logs
Terminated
